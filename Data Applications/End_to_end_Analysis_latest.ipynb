{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_1DPgqMslL_"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Colab script: run the repeated 10-fold CV experiment,\n",
        "# record individual-level predictions + squared errors.\n",
        "#\n",
        "# Output CSV (individual-level):\n",
        "#   pat_id, rep, fold, truth,\n",
        "#   lasso_pred, llm_lasso_pred, spike_slab_mcmc_fixed_mpm_pred, llm_spike_slab_mcmc_fixed_mpm_pred,\n",
        "#   lasso_sq_error, llm_lasso_sq_error, spike_slab_mcmc_fixed_mpm_sq_error, llm_spike_slab_mcmc_fixed_mpm_sq_error\n",
        "#\n",
        "# ============================================================\n",
        "\n",
        "# -------------------------\n",
        "# 0) Mount Drive + locate files\n",
        "# -------------------------\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def find_file_in_drive(filename, base=\"/content/drive/MyDrive\"):\n",
        "    \"\"\"\n",
        "    Tries:\n",
        "      1) base/filename\n",
        "      2) recursive search base/**/filename\n",
        "    Returns full path or raises.\n",
        "    \"\"\"\n",
        "    p1 = os.path.join(base, filename)\n",
        "    if os.path.exists(p1):\n",
        "        return p1\n",
        "    hits = glob.glob(os.path.join(base, \"**\", filename), recursive=True)\n",
        "    if len(hits) > 0:\n",
        "        # pick the shortest path (often the \"closest\" hit)\n",
        "        hits = sorted(hits, key=len)\n",
        "        return hits[0]\n",
        "    raise FileNotFoundError(f\"Could not find {filename} under {base}\")\n",
        "\n",
        "# EDIT only if the files are named differently\n",
        "DATA_FILENAME    = \"t60_reg_data.csv\"\n",
        "WEIGHTS_FILENAME = \"aki_weights_60.csv\"\n",
        "\n",
        "DATA_PATH    = find_file_in_drive(DATA_FILENAME)\n",
        "WEIGHTS_PATH = find_file_in_drive(WEIGHTS_FILENAME)\n",
        "\n",
        "# Where to save outputs\n",
        "OUT_DIR = \"/content/drive/MyDrive\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "ID_COL = \"pat_id\"\n",
        "TARGET = \"creatinine_t60\"\n",
        "\n",
        "print(\"DATA_PATH   =\", DATA_PATH)\n",
        "print(\"WEIGHTS_PATH=\", WEIGHTS_PATH)\n",
        "print(\"OUT_DIR     =\", OUT_DIR)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load + subset\n",
        "# -------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "wdf = pd.read_csv(WEIGHTS_PATH)\n",
        "\n",
        "need_cols = [\"pvd\", \"current_smoker\", \"former_smoker\"]\n",
        "missing = [c for c in need_cols + [ID_COL, TARGET] if c not in df.columns]\n",
        "assert len(missing) == 0, f\"Missing required columns in data: {missing}\"\n",
        "\n",
        "df = df[(df[\"pvd\"] == 1) & ((df[\"current_smoker\"] == 1) | (df[\"former_smoker\"] == 1))].copy()\n",
        "df = df.drop(columns=[\"pvd\", \"current_smoker\", \"former_smoker\"], errors=\"ignore\")\n",
        "print(\"[subset] n =\", len(df), \"| p_total (incl target/id) =\", df.shape[1])\n",
        "\n",
        "# -------------------------\n",
        "# 2) Normalize weight columns to: value, importance\n",
        "# -------------------------\n",
        "if \"value\" not in wdf.columns:\n",
        "    for cand in [\"variable_name\", \"feature\", \"name\"]:\n",
        "        if cand in wdf.columns:\n",
        "            wdf = wdf.rename(columns={cand: \"value\"})\n",
        "            break\n",
        "if \"importance\" not in wdf.columns:\n",
        "    for cand in [\"importance_score\", \"score\", \"weight\"]:\n",
        "        if cand in wdf.columns:\n",
        "            wdf = wdf.rename(columns={cand: \"importance\"})\n",
        "            break\n",
        "assert \"value\" in wdf.columns and \"importance\" in wdf.columns, f\"weights columns are {wdf.columns.tolist()}\"\n",
        "\n",
        "FEATURES = [c for c in df.columns if c not in [ID_COL, TARGET]]\n",
        "P_FULL = len(FEATURES)\n",
        "print(\"[design] n =\", len(df), \"p =\", P_FULL)\n",
        "\n",
        "# -------------------------\n",
        "# 3) LLM importance calibration\n",
        "# -------------------------\n",
        "def calibrate_importance_to_01_to_1(raw_imp: np.ndarray, min_val=0.1) -> np.ndarray:\n",
        "    x = np.asarray(raw_imp, float)\n",
        "    lo, hi = np.nanmin(x), np.nanmax(x)\n",
        "    if not np.isfinite(lo) or not np.isfinite(hi) or (hi - lo) < 1e-12:\n",
        "        return np.ones_like(x)\n",
        "    z = (x - lo) / (hi - lo)\n",
        "    return min_val + (1.0 - min_val) * z\n",
        "\n",
        "def penalty_factors_from_importance(imp_01_1: np.ndarray, power: float, eps=1e-6) -> np.ndarray:\n",
        "    imp = np.clip(np.asarray(imp_01_1, float), eps, None)\n",
        "    pen = (1.0 / imp) ** power\n",
        "    return pen / np.mean(pen)\n",
        "\n",
        "wmap = dict(zip(wdf[\"value\"].astype(str), wdf[\"importance\"].astype(float)))\n",
        "raw_imp_all = np.array([wmap.get(f, np.nan) for f in FEATURES], dtype=float)\n",
        "match_rate = np.isfinite(raw_imp_all).mean()\n",
        "print(f\"[LLM weight alignment] match_rate={match_rate:.3f}  missing={(~np.isfinite(raw_imp_all)).sum()}/{P_FULL}\")\n",
        "\n",
        "fallback = np.nanmedian(raw_imp_all) if np.isfinite(np.nanmedian(raw_imp_all)) else 1.0\n",
        "raw_imp_all = np.where(np.isfinite(raw_imp_all), raw_imp_all, fallback)\n",
        "imp_all_01_1 = calibrate_importance_to_01_to_1(raw_imp_all, min_val=0.1)\n",
        "imp_map_full = {f: imp_all_01_1[i] for i, f in enumerate(FEATURES)}\n",
        "\n",
        "# -------------------------\n",
        "# 4) Fold preprocessing (train-only stats)\n",
        "# -------------------------\n",
        "def preprocess_fold(df_train: pd.DataFrame, df_val: pd.DataFrame, feature_names):\n",
        "    Xtr_df = df_train[feature_names].copy()\n",
        "    Xva_df = df_val[feature_names].copy()\n",
        "\n",
        "    ytr = df_train[TARGET].values.reshape(-1, 1)\n",
        "    yva = df_val[TARGET].values.reshape(-1, 1)\n",
        "\n",
        "    imp = SimpleImputer(strategy=\"mean\")\n",
        "    xs = StandardScaler(with_mean=True, with_std=True)\n",
        "    ys = StandardScaler(with_mean=True, with_std=True)\n",
        "\n",
        "    Xtr = xs.fit_transform(imp.fit_transform(Xtr_df))\n",
        "    Xva = xs.transform(imp.transform(Xva_df))\n",
        "\n",
        "    ytr_s = ys.fit_transform(ytr).ravel()\n",
        "    yva_s = ys.transform(yva).ravel()\n",
        "\n",
        "    sd = Xtr.std(axis=0)\n",
        "    keep = sd > 1e-12\n",
        "    kept_idx = np.where(keep)[0]\n",
        "    kept_feats = [feature_names[i] for i in kept_idx]\n",
        "\n",
        "    Xtr = Xtr[:, keep]\n",
        "    Xva = Xva[:, keep]\n",
        "    return Xtr, ytr_s, Xva, yva_s, ys, kept_feats, kept_idx\n",
        "\n",
        "# -------------------------\n",
        "# 5) Screening: top-200 abs corr (TRAIN-only)\n",
        "# -------------------------\n",
        "def corr_abs_all(Xtr, ytr_s):\n",
        "    y = np.asarray(ytr_s).ravel()\n",
        "    Xc = Xtr - Xtr.mean(axis=0, keepdims=True)\n",
        "    yc = y - y.mean()\n",
        "    denom = (np.sqrt((Xc * Xc).sum(axis=0)) * np.sqrt((yc * yc).sum()) + 1e-12)\n",
        "    corr = (Xc.T @ yc) / denom\n",
        "    return np.abs(corr)\n",
        "\n",
        "def topk_corr_indices(Xtr, ytr_s, k=200):\n",
        "    score = corr_abs_all(Xtr, ytr_s)\n",
        "    k = int(min(k, Xtr.shape[1]))\n",
        "    idx = np.argsort(score)[::-1][:k]\n",
        "    return np.sort(idx), score\n",
        "\n",
        "# -------------------------\n",
        "# 6) Lasso + LLM-Lasso\n",
        "# -------------------------\n",
        "LASSO_ALPHAS = np.logspace(-4, 0.5, 30)\n",
        "POWERS = [0.0, 0.5, 1.0, 2.0, 3.0]\n",
        "\n",
        "def fit_lasso_fold(Xtr, ytr, Xva, alphas, inner_cv=5, seed=0):\n",
        "    model = LassoCV(alphas=alphas, cv=inner_cv, random_state=seed, max_iter=20000, fit_intercept=True)\n",
        "    model.fit(Xtr, ytr)\n",
        "    return {\"pred\": model.predict(Xva), \"coef\": model.coef_.copy(), \"alpha\": float(model.alpha_)}\n",
        "\n",
        "def _cv_mse_at_alpha(model: LassoCV) -> float:\n",
        "    i_star = np.where(model.alphas_ == model.alpha_)[0]\n",
        "    if len(i_star) == 0:\n",
        "        return float(np.mean(np.min(model.mse_path_, axis=0)))\n",
        "    return float(np.mean(model.mse_path_[i_star[0], :]))\n",
        "\n",
        "def fit_llm_lasso_fold(Xtr, ytr, Xva, imp_01_1_feats, powers, alphas, inner_cv=5, seed=0):\n",
        "    best = {\"cv_score\": np.inf}\n",
        "    for pwr in powers:\n",
        "        pen = penalty_factors_from_importance(imp_01_1_feats, power=float(pwr))\n",
        "        Xtr_w = Xtr / pen\n",
        "        Xva_w = Xva / pen\n",
        "\n",
        "        model = LassoCV(alphas=alphas, cv=inner_cv, random_state=seed, max_iter=20000, fit_intercept=True)\n",
        "        model.fit(Xtr_w, ytr)\n",
        "        cv_mse = _cv_mse_at_alpha(model)\n",
        "\n",
        "        beta = model.coef_ / pen\n",
        "        pred_va = float(model.intercept_) + Xva.dot(beta)\n",
        "\n",
        "        if cv_mse < best[\"cv_score\"]:\n",
        "            best = {\n",
        "                \"pred\": pred_va,\n",
        "                \"coef\": beta.copy(),\n",
        "                \"alpha\": float(model.alpha_),\n",
        "                \"power\": float(pwr),\n",
        "                \"cv_score\": float(cv_mse),\n",
        "                \"pen\": pen.copy(),\n",
        "            }\n",
        "    return best\n",
        "\n",
        "# -------------------------\n",
        "# 7) SSVS Gibbs + K-only inner CV + Top-K refit\n",
        "# -------------------------\n",
        "def importance_to_pi_raw(imp_01_1, pi_min=0.001, pi_max=0.90, gamma=6.0):\n",
        "    imp = np.asarray(imp_01_1, float)\n",
        "    lo, hi = float(np.min(imp)), float(np.max(imp))\n",
        "    if hi - lo < 1e-12:\n",
        "        z = np.full_like(imp, 0.5, dtype=float)\n",
        "    else:\n",
        "        z = (imp - lo) / (hi - lo)\n",
        "    z = np.clip(z, 0.0, 1.0) ** float(gamma)\n",
        "    return pi_min + (pi_max - pi_min) * z\n",
        "\n",
        "def rescale_pi_to_mean_logit(pi_raw, target_mean, eps=1e-6):\n",
        "    pi_raw = np.clip(np.asarray(pi_raw, float), eps, 1 - eps)\n",
        "    z = np.log(pi_raw / (1 - pi_raw))\n",
        "    lo, hi = -20.0, 20.0\n",
        "    for _ in range(60):\n",
        "        mid = 0.5 * (lo + hi)\n",
        "        m = float(np.mean(1.0 / (1.0 + np.exp(-(z + mid)))))\n",
        "        if m > target_mean:\n",
        "            hi = mid\n",
        "        else:\n",
        "            lo = mid\n",
        "    a = 0.5 * (lo + hi)\n",
        "    pi = 1.0 / (1.0 + np.exp(-(z + a)))\n",
        "    return np.clip(pi, eps, 1 - eps)\n",
        "\n",
        "def make_pi_baseline(p_used, K_expected):\n",
        "    pi0 = np.full(p_used, float(K_expected) / max(p_used, 1), dtype=float)\n",
        "    return np.clip(pi0, 1e-6, 1 - 1e-6)\n",
        "\n",
        "def make_pi_llm(imp_vec, K_expected, gamma_llm, pi_min=0.001, pi_max=0.90):\n",
        "    pi_raw = importance_to_pi_raw(imp_vec, pi_min=pi_min, pi_max=pi_max, gamma=float(gamma_llm))\n",
        "    target_mean = float(K_expected) / max(len(imp_vec), 1)\n",
        "    return rescale_pi_to_mean_logit(pi_raw, target_mean)\n",
        "\n",
        "def ssvs_gibbs_coordinate_sigma_scaled(\n",
        "    X, y, pi,\n",
        "    tau0=0.01, tau1=1.0,\n",
        "    a0=1e-2, b0=1e-2,\n",
        "    n_iter=2200, burn=1000, thin=2,\n",
        "    seed=0\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n, p = X.shape\n",
        "    eps = 1e-12\n",
        "    pi = np.clip(np.asarray(pi, float).ravel(), 1e-6, 1 - 1e-6)\n",
        "\n",
        "    beta = np.zeros(p, dtype=float)\n",
        "    sigma2 = 1.0\n",
        "    gamma = rng.binomial(1, pi, size=p).astype(int)\n",
        "\n",
        "    r = y.copy()\n",
        "    x2 = (X * X).sum(axis=0) + eps\n",
        "\n",
        "    beta_sum = np.zeros(p, dtype=float)\n",
        "    gamma_sum = np.zeros(p, dtype=float)\n",
        "    n_save = 0\n",
        "\n",
        "    for it in range(int(n_iter)):\n",
        "        v1 = sigma2 * (tau1 ** 2)\n",
        "        v0 = sigma2 * (tau0 ** 2)\n",
        "\n",
        "        logp1 = np.log(pi + eps) - 0.5 * np.log(v1 + eps) - (beta ** 2) / (2.0 * (v1 + eps))\n",
        "        logp0 = np.log(1.0 - pi + eps) - 0.5 * np.log(v0 + eps) - (beta ** 2) / (2.0 * (v0 + eps))\n",
        "        prob1 = 1.0 / (1.0 + np.exp(logp0 - logp1))\n",
        "        gamma = rng.binomial(1, prob1).astype(int)\n",
        "\n",
        "        for j in range(p):\n",
        "            r += X[:, j] * beta[j]\n",
        "            tau_j = tau1 if gamma[j] == 1 else tau0\n",
        "            prior_var = sigma2 * (tau_j ** 2) + eps\n",
        "            post_var = 1.0 / (x2[j] / sigma2 + 1.0 / prior_var)\n",
        "            post_mean = post_var * (X[:, j].dot(r) / sigma2)\n",
        "            beta[j] = rng.normal(post_mean, np.sqrt(post_var))\n",
        "            r -= X[:, j] * beta[j]\n",
        "\n",
        "        rss = float((r * r).sum())\n",
        "        shape = a0 + n / 2.0\n",
        "        scale = b0 + 0.5 * rss\n",
        "        sigma2 = 1.0 / rng.gamma(shape, 1.0 / scale)\n",
        "\n",
        "        if it >= burn and ((it - burn) % thin == 0):\n",
        "            beta_sum += beta\n",
        "            gamma_sum += gamma\n",
        "            n_save += 1\n",
        "\n",
        "    beta_mean = beta_sum / max(n_save, 1)\n",
        "    pip = gamma_sum / max(n_save, 1)\n",
        "    return beta_mean, pip\n",
        "\n",
        "def fit_ssvs_single(Xtr, ytr, pi, tau0, tau1, n_iter, burn, thin, seed):\n",
        "    beta, pip = ssvs_gibbs_coordinate_sigma_scaled(\n",
        "        Xtr, ytr, pi,\n",
        "        tau0=float(tau0), tau1=float(tau1),\n",
        "        n_iter=int(n_iter), burn=int(burn), thin=int(thin),\n",
        "        seed=int(seed)\n",
        "    )\n",
        "    return beta, pip\n",
        "\n",
        "def inner_cv_pick_ssvs_K_only(\n",
        "    Xtr_s, ytr_s, imp_scr,\n",
        "    is_llm: bool,\n",
        "    K_grid,\n",
        "    tau0=0.01,\n",
        "    tau1_fixed=1.0,\n",
        "    gamma_llm_fixed=6.0,\n",
        "    inner_splits=2,\n",
        "    n_iter=900, burn=400, thin=2,\n",
        "    base_seed=0\n",
        "):\n",
        "    n = Xtr_s.shape[0]\n",
        "    kf_in = KFold(n_splits=int(inner_splits), shuffle=True, random_state=int(base_seed))\n",
        "    best = {\"cv_mse\": np.inf}\n",
        "\n",
        "    for Kexp in K_grid:\n",
        "        mses = []\n",
        "        for i, (i_tr, i_va) in enumerate(kf_in.split(np.arange(n))):\n",
        "            Xin_tr, yin_tr = Xtr_s[i_tr], ytr_s[i_tr]\n",
        "            Xin_va, yin_va = Xtr_s[i_va], ytr_s[i_va]\n",
        "\n",
        "            if is_llm:\n",
        "                pi = make_pi_llm(imp_scr, int(Kexp), gamma_llm=float(gamma_llm_fixed))\n",
        "                g_term = int(10 * float(gamma_llm_fixed))\n",
        "            else:\n",
        "                pi = make_pi_baseline(Xtr_s.shape[1], int(Kexp))\n",
        "                g_term = 0\n",
        "\n",
        "            seed = (\n",
        "                1_000_000\n",
        "                + int(base_seed) * 10_000\n",
        "                + int(Kexp) * 101\n",
        "                + int(round(100 * float(tau1_fixed))) * 17\n",
        "                + 97 * i\n",
        "                + g_term\n",
        "            )\n",
        "\n",
        "            beta, _ = fit_ssvs_single(\n",
        "                Xin_tr, yin_tr, pi,\n",
        "                tau0=tau0, tau1=float(tau1_fixed),\n",
        "                n_iter=n_iter, burn=burn, thin=thin,\n",
        "                seed=seed\n",
        "            )\n",
        "            pred = Xin_va @ beta\n",
        "            mses.append(mean_squared_error(yin_va, pred))\n",
        "\n",
        "        cv_mse = float(np.mean(mses))\n",
        "        if cv_mse < best[\"cv_mse\"]:\n",
        "            best = {\n",
        "                \"cv_mse\": cv_mse,\n",
        "                \"K_expected\": int(Kexp),\n",
        "                \"tau1\": float(tau1_fixed),\n",
        "                \"gamma_llm\": (None if not is_llm else float(gamma_llm_fixed)),\n",
        "            }\n",
        "    return best\n",
        "\n",
        "def topk_pip_refit_beta(Xtr, ytr, pip, k, ridge=1e-6):\n",
        "    pip = np.asarray(pip, float).ravel()\n",
        "    p = pip.shape[0]\n",
        "    k = int(max(0, min(int(k), p)))\n",
        "    beta_full = np.zeros(p, dtype=float)\n",
        "    if k == 0:\n",
        "        return beta_full, 0\n",
        "    S = np.argsort(pip)[::-1][:k]\n",
        "    S = np.sort(S)\n",
        "    Xs = Xtr[:, S]\n",
        "    XtX = Xs.T @ Xs + float(ridge) * np.eye(len(S))\n",
        "    Xty = Xs.T @ ytr\n",
        "    beta_s = np.linalg.solve(XtX, Xty)\n",
        "    beta_full[S] = beta_s\n",
        "    return beta_full, int(len(S))\n",
        "\n",
        "# -------------------------\n",
        "# 8) Settings\n",
        "# -------------------------\n",
        "K_FOLDS = 10\n",
        "N_REPEATS = 5\n",
        "BASE_SEED = 0\n",
        "\n",
        "TOPK = 200\n",
        "\n",
        "TAU0 = 0.01\n",
        "TAU1_FIXED = 1.0\n",
        "GAMMA_LLM_FIXED = 6.0\n",
        "\n",
        "OUT_NITER = 2200\n",
        "OUT_BURN  = 1000\n",
        "OUT_THIN  = 2\n",
        "\n",
        "IN_NITER = 900\n",
        "IN_BURN  = 400\n",
        "IN_THIN  = 2\n",
        "INNER_SPLITS = 2\n",
        "\n",
        "K_GRID = [10]\n",
        "\n",
        "assert len(df) >= K_FOLDS, f\"Subset too small for {K_FOLDS}-fold CV: n={len(df)}\"\n",
        "\n",
        "# -------------------------\n",
        "# 9) Run CV + record individual preds\n",
        "# -------------------------\n",
        "fold_rows = []\n",
        "chosen_rows = []\n",
        "ind_pred_chunks = []  # list of per-fold DataFrames (individual-level)\n",
        "\n",
        "for rep in range(N_REPEATS):\n",
        "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=BASE_SEED + rep)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(kf.split(df), start=1):\n",
        "        df_tr = df.iloc[tr_idx].copy()\n",
        "        df_va = df.iloc[va_idx].copy()\n",
        "\n",
        "        Xtr, ytr_s, Xva, yva_s, y_scaler, kept_feats, kept_idx = preprocess_fold(df_tr, df_va, FEATURES)\n",
        "\n",
        "        # TRAIN-only screening: top-200 corr\n",
        "        idx_top, _ = topk_corr_indices(Xtr, ytr_s, k=TOPK)\n",
        "\n",
        "        Xtr_s = Xtr[:, idx_top]\n",
        "        Xva_s = Xva[:, idx_top]\n",
        "        feats_s = [kept_feats[i] for i in idx_top]\n",
        "        p_used = Xtr_s.shape[1]\n",
        "\n",
        "        imp_scr = np.array([imp_map_full.get(f, 0.5) for f in feats_s], dtype=float)\n",
        "\n",
        "        # (1) Lasso\n",
        "        lasso_fit = fit_lasso_fold(\n",
        "            Xtr_s, ytr_s, Xva_s,\n",
        "            alphas=LASSO_ALPHAS, inner_cv=5,\n",
        "            seed=10_000 + rep * 100 + fold\n",
        "        )\n",
        "\n",
        "        # (2) LLM-Lasso\n",
        "        llm_lasso_fit = fit_llm_lasso_fold(\n",
        "            Xtr_s, ytr_s, Xva_s, imp_scr,\n",
        "            powers=POWERS, alphas=LASSO_ALPHAS,\n",
        "            inner_cv=5, seed=20_000 + rep * 100 + fold\n",
        "        )\n",
        "\n",
        "        # Inner CV (K-only): baseline SSVS\n",
        "        best_base = inner_cv_pick_ssvs_K_only(\n",
        "            Xtr_s, ytr_s, imp_scr,\n",
        "            is_llm=False,\n",
        "            K_grid=K_GRID,\n",
        "            tau0=TAU0,\n",
        "            tau1_fixed=TAU1_FIXED,\n",
        "            gamma_llm_fixed=GAMMA_LLM_FIXED,\n",
        "            inner_splits=INNER_SPLITS,\n",
        "            n_iter=IN_NITER, burn=IN_BURN, thin=IN_THIN,\n",
        "            base_seed=30_000 + rep * 100 + fold\n",
        "        )\n",
        "\n",
        "        # Inner CV (K-only): LLM SSVS\n",
        "        best_llm = inner_cv_pick_ssvs_K_only(\n",
        "            Xtr_s, ytr_s, imp_scr,\n",
        "            is_llm=True,\n",
        "            K_grid=K_GRID,\n",
        "            tau0=TAU0,\n",
        "            tau1_fixed=TAU1_FIXED,\n",
        "            gamma_llm_fixed=GAMMA_LLM_FIXED,\n",
        "            inner_splits=INNER_SPLITS,\n",
        "            n_iter=IN_NITER, burn=IN_BURN, thin=IN_THIN,\n",
        "            base_seed=40_000 + rep * 100 + fold\n",
        "        )\n",
        "\n",
        "        # Fit outer TRAIN: baseline SSVS\n",
        "        pi0 = make_pi_baseline(p_used, best_base[\"K_expected\"])\n",
        "        beta_ss_mean, pip_ss = fit_ssvs_single(\n",
        "            Xtr_s, ytr_s, pi0,\n",
        "            tau0=TAU0, tau1=TAU1_FIXED,\n",
        "            n_iter=OUT_NITER, burn=OUT_BURN, thin=OUT_THIN,\n",
        "            seed=50_000 + rep * 100 + fold\n",
        "        )\n",
        "\n",
        "        # Fit outer TRAIN: LLM SSVS\n",
        "        pi_llm = make_pi_llm(imp_scr, best_llm[\"K_expected\"], gamma_llm=GAMMA_LLM_FIXED)\n",
        "        beta_llm_mean, pip_llm = fit_ssvs_single(\n",
        "            Xtr_s, ytr_s, pi_llm,\n",
        "            tau0=TAU0, tau1=TAU1_FIXED,\n",
        "            n_iter=OUT_NITER, burn=OUT_BURN, thin=OUT_THIN,\n",
        "            seed=60_000 + rep * 100 + fold\n",
        "        )\n",
        "\n",
        "        # TRAIN-only refit using Top-K PIPs (K_expected)  [NO pip>=0.5 for prediction]\n",
        "        beta_ss_refit,  size_base = topk_pip_refit_beta(Xtr_s, ytr_s, pip_ss,  k=best_base[\"K_expected\"], ridge=1e-6)\n",
        "        beta_llm_refit, size_llm  = topk_pip_refit_beta(Xtr_s, ytr_s, pip_llm, k=best_llm[\"K_expected\"],  ridge=1e-6)\n",
        "\n",
        "        # In original units\n",
        "        y_true = y_scaler.inverse_transform(yva_s.reshape(-1, 1)).ravel()\n",
        "        def inv(pred_s):\n",
        "            return y_scaler.inverse_transform(np.asarray(pred_s).reshape(-1, 1)).ravel()\n",
        "\n",
        "        preds = {\n",
        "            \"lasso\": inv(lasso_fit[\"pred\"]),\n",
        "            \"llm_lasso\": inv(llm_lasso_fit[\"pred\"]),\n",
        "            \"spike_slab_mcmc_fixed_mpm\": inv(Xva_s @ beta_ss_refit),\n",
        "            \"llm_spike_slab_mcmc_fixed_mpm\": inv(Xva_s @ beta_llm_refit),\n",
        "        }\n",
        "\n",
        "        # ---- individual-level table ----\n",
        "        ind_df = pd.DataFrame({\n",
        "            \"rep\": rep,\n",
        "            \"fold\": fold,\n",
        "            \"row_index\": df_va.index.values,          # original row index within df after subsetting\n",
        "            ID_COL: df_va[ID_COL].values,\n",
        "            \"truth\": y_true,\n",
        "            \"lasso_pred\": preds[\"lasso\"],\n",
        "            \"llm_lasso_pred\": preds[\"llm_lasso\"],\n",
        "            \"spike_slab_mcmc_fixed_mpm_pred\": preds[\"spike_slab_mcmc_fixed_mpm\"],\n",
        "            \"llm_spike_slab_mcmc_fixed_mpm_pred\": preds[\"llm_spike_slab_mcmc_fixed_mpm\"],\n",
        "        })\n",
        "        # squared errors\n",
        "        ind_df[\"lasso_sq_error\"] = (ind_df[\"lasso_pred\"] - ind_df[\"truth\"]) ** 2\n",
        "        ind_df[\"llm_lasso_sq_error\"] = (ind_df[\"llm_lasso_pred\"] - ind_df[\"truth\"]) ** 2\n",
        "        ind_df[\"spike_slab_mcmc_fixed_mpm_sq_error\"] = (ind_df[\"spike_slab_mcmc_fixed_mpm_pred\"] - ind_df[\"truth\"]) ** 2\n",
        "        ind_df[\"llm_spike_slab_mcmc_fixed_mpm_sq_error\"] = (ind_df[\"llm_spike_slab_mcmc_fixed_mpm_pred\"] - ind_df[\"truth\"]) ** 2\n",
        "\n",
        "        ind_pred_chunks.append(ind_df)\n",
        "\n",
        "        # ---- fold-level perf summary ----\n",
        "        for m, yhat in preds.items():\n",
        "            fold_rows.append({\n",
        "                \"rep\": rep,\n",
        "                \"fold\": fold,\n",
        "                \"method\": m,\n",
        "                \"n_train\": int(len(df_tr)),\n",
        "                \"n_val\": int(len(df_va)),\n",
        "                \"p_used\": int(p_used),\n",
        "                \"mse\": float(mean_squared_error(y_true, yhat)),\n",
        "                \"r2\": float(r2_score(y_true, yhat)),\n",
        "            })\n",
        "\n",
        "        chosen_rows.append({\n",
        "            \"rep\": rep, \"fold\": fold, \"p_used\": int(p_used),\n",
        "            \"base_K\": best_base[\"K_expected\"], \"base_cv_mse\": best_base[\"cv_mse\"],\n",
        "            \"llm_K\": best_llm[\"K_expected\"], \"llm_cv_mse\": best_llm[\"cv_mse\"],\n",
        "            \"tau1_fixed\": float(TAU1_FIXED),\n",
        "            \"gamma_llm_fixed\": float(GAMMA_LLM_FIXED),\n",
        "            \"refit_rule\": \"Top-K PIP refit (K_expected)\",\n",
        "            \"refit_size_base\": int(size_base),\n",
        "            \"refit_size_llm\": int(size_llm),\n",
        "            \"llm_lasso_power\": float(llm_lasso_fit[\"power\"]),\n",
        "            \"lasso_alpha\": float(lasso_fit[\"alpha\"]),\n",
        "            \"llm_lasso_alpha\": float(llm_lasso_fit[\"alpha\"]),\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"done rep={rep} fold={fold} | n={len(df_tr)}/{len(df_va)} p_used={p_used} \"\n",
        "            f\"| base(K={best_base['K_expected']},refit={size_base}) \"\n",
        "            f\"| llm(K={best_llm['K_expected']},refit={size_llm})\",\n",
        "            flush=True\n",
        "        )\n",
        "\n",
        "    print(f\"[repeat {rep+1}/{N_REPEATS}] done\", flush=True)\n",
        "\n",
        "perf   = pd.DataFrame(fold_rows)\n",
        "chosen = pd.DataFrame(chosen_rows)\n",
        "indiv  = pd.concat(ind_pred_chunks, ignore_index=True)\n",
        "\n",
        "print(\"\\n=== Mean outer-fold performance ===\")\n",
        "print(perf.groupby(\"method\")[[\"mse\", \"r2\"]].mean().sort_values(\"mse\"))\n",
        "\n",
        "# -------------------------\n",
        "# 10) Performance SE (fold-level)\n",
        "#\n",
        "# -------------------------\n",
        "def se(x):\n",
        "    x = pd.to_numeric(x, errors=\"coerce\").dropna().to_numpy()\n",
        "    n = len(x)\n",
        "    if n <= 1:\n",
        "        return np.nan\n",
        "    return float(np.std(x, ddof=1) / np.sqrt(n))\n",
        "\n",
        "def summarize_perf_with_se(perf_df):\n",
        "    out = []\n",
        "    for method, sub in perf_df.groupby(\"method\", dropna=False):\n",
        "        row = {\"method\": method}\n",
        "        for m in [\"mse\", \"r2\"]:\n",
        "            vals = pd.to_numeric(sub[m], errors=\"coerce\")\n",
        "            row[f\"{m}_n\"] = int(vals.notna().sum())\n",
        "            row[f\"{m}_mean\"] = float(vals.mean())\n",
        "            row[f\"{m}_std\"]  = float(vals.std(ddof=1))\n",
        "            row[f\"{m}_se\"]   = se(vals)\n",
        "        out.append(row)\n",
        "    return pd.DataFrame(out).sort_values(\"method\").reset_index(drop=True)\n",
        "\n",
        "perf_se = summarize_perf_with_se(perf)\n",
        "print(\"\\n=== PERF: mean/std/SE over all outer folds (rep*fold rows) ===\")\n",
        "print(perf_se)\n",
        "\n",
        "# -------------------------\n",
        "# 11) Save outputs to Drive\n",
        "# -------------------------\n",
        "OUT_PERF   = os.path.join(OUT_DIR, \"aki_t60_subset_pvd_smoker_repeated10fold_perf_4methods_top200corr_fixedTauGamma_refitTopK.csv\")\n",
        "OUT_CHOSEN = os.path.join(OUT_DIR, \"aki_t60_subset_pvd_smoker_repeated10fold_chosen_hparams_fixedTauGamma_refitTopK.csv\")\n",
        "OUT_INDIV  = os.path.join(OUT_DIR, \"aki_t60_subset_pvd_smoker_individual_level_predictions_and_sq_errors.csv\")\n",
        "OUT_PERFSE = os.path.join(OUT_DIR, \"aki_t60_subset_pvd_smoker_perf_summary_with_se.csv\")\n",
        "\n",
        "perf.to_csv(OUT_PERF, index=False)\n",
        "chosen.to_csv(OUT_CHOSEN, index=False)\n",
        "indiv.to_csv(OUT_INDIV, index=False)\n",
        "perf_se.to_csv(OUT_PERFSE, index=False)\n"
      ]
    }
  ]
}