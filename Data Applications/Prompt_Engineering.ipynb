{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWor8RI3U-Wo"
      },
      "outputs": [],
      "source": [
        "# --- AKI (t24) LLM score collection: structured outputs + batching + calibration\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 0) Load data + columns\n",
        "# -----------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/t60_reg_data.csv\"  # <-- change to your local path\n",
        "ID_COLS = [\"pat_id\"]\n",
        "TARGET_COL = \"creatinine_t60\"   # regression target\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "FEATURE_COLS = [c for c in df.columns if c not in ID_COLS + [TARGET_COL]]\n",
        "print(\"n_features =\", len(FEATURE_COLS))\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 1) Prompt scaffolding\n",
        "# -----------------------\n",
        "DATASET_BACKGROUND = \"\"\"\n",
        "We have adult patients who underwent cardiac surgery.\n",
        "For each patient, we have perioperative and ICU EMR-derived features:\n",
        "demographics, comorbidities, vitals/hemodynamics, ventilator settings,\n",
        "fluid balance, labs, procedures, and medication doses.\n",
        "The modeling goal is sparse linear regression to predict postoperative\n",
        "serum creatinine at 60 hours (creatinine_t60).\n",
        "\"\"\".strip()\n",
        "\n",
        "OUTCOME_NAME = \"postoperative serum creatinine at 60 hours (creatinine_t60)\"\n",
        "\n",
        "def infer_feature_type(series: pd.Series) -> str:\n",
        "    \"\"\"Light heuristic for the LLM (helps it reason about binary flags vs continuous).\"\"\"\n",
        "    if pd.api.types.is_bool_dtype(series):\n",
        "        return \"binary\"\n",
        "    if pd.api.types.is_numeric_dtype(series):\n",
        "        vals = series.dropna().unique()\n",
        "        if len(vals) <= 3 and set(vals).issubset({0, 1}):\n",
        "            return \"binary\"\n",
        "        return \"numeric\"\n",
        "    return \"categorical_or_text\"\n",
        "\n",
        "def build_prompt_for_batch(feature_names: List[str]) -> str:\n",
        "    # Include a tiny bit of metadata that is cheap but useful:\n",
        "    payload = []\n",
        "    for i, name in enumerate(feature_names):\n",
        "        ftype = infer_feature_type(df[name])\n",
        "        payload.append({\"id\": i, \"name\": name, \"type\": ftype})\n",
        "\n",
        "    features_json = json.dumps(payload, indent=2)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a cardiothoracic ICU clinician and a biostatistician.\n",
        "\n",
        "Background:\n",
        "{DATASET_BACKGROUND}\n",
        "\n",
        "Task:\n",
        "We are building a sparse linear regression model to predict {OUTCOME_NAME}.\n",
        "Each input is a feature (column) from the tabular dataset.\n",
        "\n",
        "For each feature, rate how a priori clinically relevant it is for predicting {OUTCOME_NAME}\n",
        "in adult postcardiac-surgery ICU patients. Base your judgment on typical knowledge about:\n",
        "kidney perfusion/hemodynamics, AKI risk factors, nephrotoxic medications, and kidney-related labs.\n",
        "\n",
        "Scoring rules (importance):\n",
        "- Use an integer score from 1 to 5:\n",
        "  1 = very unlikely to be useful\n",
        "  2 = weak/indirect relevance\n",
        "  3 = moderately relevant\n",
        "  4 = clearly relevant\n",
        "  5 = directly and strongly related to kidney function or AKI mechanisms\n",
        "\n",
        "Input features (JSON list):\n",
        "{features_json}\n",
        "\n",
        "Return a JSON object with key \"scores\" containing a list of objects.\n",
        "Each object must include:\n",
        "- id (same as input id)\n",
        "- name (copy exactly)\n",
        "- importance (integer 1..5)\n",
        "- reason (1–2 concise sentences)\n",
        "\"\"\".strip()\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# 2) Structured output schema (Pydantic)\n",
        "# -----------------------------------\n",
        "class ScoreItem(BaseModel):\n",
        "    id: int\n",
        "    name: str\n",
        "    importance: int = Field(ge=1, le=5)\n",
        "    reason: str\n",
        "\n",
        "class ScoreBatch(BaseModel):\n",
        "    scores: List[ScoreItem]\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 3) OpenAI client + call\n",
        "# -----------------------\n",
        "# Set env var: export OPENAI_API_KEY=\"...\"\n",
        "client = OpenAI(api_key=\"sk-vJk8GQyC63N0G8cB5t1P-AFz0L62KMvAxmpBhaoldPT3BlbkFJAWwkbEOrbZCTd06GiwrcJBM24oTugZbuNwnlFSFQMA\")\n",
        "\n",
        "def call_llm_for_batch(prompt: str, model: str = \"gpt-4o-2024-08-06\") -> ScoreBatch:\n",
        "    \"\"\"\n",
        "    Uses Responses API structured parsing, so you don't need to manually parse JSON text.\n",
        "    \"\"\"\n",
        "    resp = client.responses.parse(\n",
        "        model=model,\n",
        "        input=[\n",
        "            {\"role\": \"system\", \"content\": \"You output only structured JSON that matches the schema.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        text_format=ScoreBatch,\n",
        "        temperature=0,\n",
        "        max_output_tokens=2000,\n",
        "    )\n",
        "    return resp.output_parsed\n",
        "\n",
        "\n",
        "def get_llm_importance_scores(\n",
        "    feature_names: List[str],\n",
        "    batch_size: Optional[int] = None,\n",
        "    model: str = \"gpt-4o-2024-08-06\",\n",
        "    sleep_s: float = 0.25,\n",
        "    max_retries: int = 6,\n",
        "    cache_path: str = \"aki_llm_scores_raw.json\",\n",
        ") -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      dict[feature_name] = {\"importance\": int(1..5), \"reason\": str}\n",
        "    Caches incrementally so you can resume.\n",
        "    \"\"\"\n",
        "\n",
        "    # default: ceil(sqrt(p)) like the paper’s heuristic\n",
        "    if batch_size is None:\n",
        "        batch_size = int(math.ceil(math.sqrt(len(feature_names))))\n",
        "\n",
        "    # load cache if exists\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "        print(f\"[cache] loaded {len(results)} scores from {cache_path}\")\n",
        "    else:\n",
        "        results = {}\n",
        "\n",
        "    remaining = [f for f in feature_names if f not in results]\n",
        "    print(f\"Remaining features to score: {len(remaining)} (batch_size={batch_size})\")\n",
        "\n",
        "    for start in range(0, len(remaining), batch_size):\n",
        "        batch = remaining[start : start + batch_size]\n",
        "        prompt = build_prompt_for_batch(batch)\n",
        "\n",
        "        # retry loop (simple exponential backoff)\n",
        "        attempt = 0\n",
        "        while True:\n",
        "            try:\n",
        "                parsed = call_llm_for_batch(prompt, model=model)\n",
        "                # map back\n",
        "                for item in parsed.scores:\n",
        "                    results[item.name] = {\n",
        "                        \"importance\": int(item.importance),\n",
        "                        \"reason\": item.reason,\n",
        "                    }\n",
        "                # write cache\n",
        "                with open(cache_path, \"w\") as f:\n",
        "                    json.dump(results, f, indent=2)\n",
        "                print(f\"[ok] scored batch {start//batch_size + 1} / {math.ceil(len(remaining)/batch_size)}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                attempt += 1\n",
        "                if attempt > max_retries:\n",
        "                    raise RuntimeError(f\"Failed after {max_retries} retries. Last error: {e}\") from e\n",
        "                backoff = (2 ** attempt) * 0.5\n",
        "                print(f\"[retry {attempt}] error={e} | sleeping {backoff:.1f}s\")\n",
        "                time.sleep(backoff)\n",
        "\n",
        "        time.sleep(sleep_s)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 4) batch calibration (paper-style idea)\n",
        "# -----------------------\n",
        "def calibrate_batch_scales(\n",
        "    feature_names: List[str],\n",
        "    raw_scores: Dict[str, Dict],\n",
        "    batch_size: int,\n",
        "    model: str = \"gpt-4o-2024-08-06\",\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Produces per-feature calibrated scores in [0.1, 1.0] (useful for penalty factors).\n",
        "    Idea:\n",
        "      - group features by the same batching you used\n",
        "      - take the max-scored feature per batch\n",
        "      - ask LLM to rescore just those maxima (global comparison)\n",
        "      - weight each batch by normalized rescored maximum\n",
        "      - combine and rescale to [0.1, 1.0]\n",
        "    \"\"\"\n",
        "    # recreate batches\n",
        "    batches = [feature_names[i:i+batch_size] for i in range(0, len(feature_names), batch_size)]\n",
        "\n",
        "    # pick max feature per batch\n",
        "    max_feats = []\n",
        "    for b in batches:\n",
        "        best = max(b, key=lambda nm: raw_scores[nm][\"importance\"])\n",
        "        max_feats.append(best)\n",
        "\n",
        "    # rescore maxima globally\n",
        "    prompt = build_prompt_for_batch(max_feats)\n",
        "    parsed = call_llm_for_batch(prompt, model=model)\n",
        "\n",
        "    # batch weights from rescored maxima\n",
        "    rescored = {it.name: it.importance for it in parsed.scores}\n",
        "    s = np.array([rescored.get(f, 1) for f in max_feats], dtype=float)\n",
        "    w = s / (s.sum() + 1e-12)\n",
        "\n",
        "    # weighted concat\n",
        "    calibrated = {}\n",
        "    for bi, b in enumerate(batches):\n",
        "        for nm in b:\n",
        "            calibrated[nm] = raw_scores[nm][\"importance\"] * w[bi]\n",
        "\n",
        "    # rescale to [0.1, 1.0]\n",
        "    vals = np.array(list(calibrated.values()), dtype=float)\n",
        "    vmin, vmax = vals.min(), vals.max()\n",
        "    for k in calibrated:\n",
        "        if vmax > vmin:\n",
        "            z = (calibrated[k] - vmin) / (vmax - vmin)\n",
        "        else:\n",
        "            z = 0.0\n",
        "        calibrated[k] = 0.1 + 0.9 * z\n",
        "\n",
        "    return calibrated\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 5) Run + export to aki_weights.csv format\n",
        "# -----------------------\n",
        "MODEL = \"gpt-4o-2024-08-06\"\n",
        "\n",
        "raw = get_llm_importance_scores(\n",
        "    FEATURE_COLS,\n",
        "    batch_size=None,\n",
        "    model=MODEL,\n",
        "    cache_path=\"aki_llm_scores_raw.json\",\n",
        ")\n",
        "\n",
        "# Save raw weights (1..5)\n",
        "raw_df = pd.DataFrame({\n",
        "    \"value\": list(raw.keys()),\n",
        "    \"importance\": [raw[k][\"importance\"] for k in raw],\n",
        "    \"reason\": [raw[k][\"reason\"] for k in raw],\n",
        "})\n",
        "raw_df.to_csv(\"aki_weights_raw.csv\", index=False)\n",
        "\n",
        "# Optional calibrated weights in [0.1, 1.0]\n",
        "batch_size_used = int(math.ceil(math.sqrt(len(FEATURE_COLS))))\n",
        "cal = calibrate_batch_scales(FEATURE_COLS, raw, batch_size=batch_size_used, model=MODEL)\n",
        "\n",
        "cal_df = pd.DataFrame({\n",
        "    \"value\": list(cal.keys()),\n",
        "    \"importance\": [cal[k] for k in cal],   # now continuous in [0.1, 1.0]\n",
        "})\n",
        "cal_df.to_csv(\"aki_weights.csv\", index=False)"
      ]
    }
  ]
}